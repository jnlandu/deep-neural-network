# -*- coding: utf-8 -*-
"""jeremie_nn_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GvTF8go9yyyu1A7912yENl16xz0UhPu2

## Import basic libraries:
"""

import numpy as np
import matplotlib.pyplot as plt

"""## The class neuralNetwork:"""

class neuralNetwork:

  def __init__(self, lr,n_epochs):
    self.n_epochs = n_epochs
    self.lr = lr
    self.train_loss = []
    self.test_loss = []
    # self.h0 = 2  # Shall be passed as a parameter
    # self.h1 = 10 # Shall be passed as a parameter
    # self.h2 = 1 # Shall be passed as a parameter
    self.W1 = None
    self.W2 = None
    self.b1 = None
    self.b2 = None

  def init_params(self, h0=2,h1=10,h2=1):

    self.W1 = np.random.randn(h1,h0)
    self.W2 = np.random.randn(h2,h1)
    self.b1 = np.random.randn(h1, h2)
    self.b2 = np.random.randn(h2, h2)
    return self.W1, self.W2, self.b1, self.b2

  def sigmoid(self, z):
    return 1/(1+np.exp(-z))

  def d_sigmoid(self, z):
    return self.sigmoid(z)*(1 - self.sigmoid(z))

  def loss(self,y_pred,Y):
      loss = (-1 / Y.shape[0]) * np.sum(Y * np.log(y_pred) + (1 - Y) * np.log(1 - y_pred))
      return loss

  def forward_pass(self,X, W1,W2, b1, b2):
    Z1 =  W1.dot(X) + b1
    A1 = self.sigmoid(Z1)

    Z2 = W2.dot(A1) + b2
    A2 = self.sigmoid(Z2)
    return A2, Z2, A1, Z1

  def backward_pass(self,X,Y, A2, Z2, A1, Z1, W1, W2, b1, b2):

    dL_dA2 = (A2 - Y)/(A2 * (1-A2))
    dA2_dZ2 = self.d_sigmoid(Z2)
    dZ2_dW2 = A1.T

    dW2 = (dL_dA2 * dA2_dZ2) @ dZ2_dW2
    db2 = dL_dA2 @ dA2_dZ2.T

    dZ2_dA1 = W2
    dA1_dZ1 = self.d_sigmoid(Z1)
    dZ1_dW1 = X.T

    dW1 = (dZ2_dA1.T * (dL_dA2 * dA2_dZ2)* dA1_dZ1) @ dZ1_dW1
    db1 = ((dL_dA2 * dA2_dZ2)@(dZ2_dA1.T *dA1_dZ1).T).T

    return  dW1, dW2, db1, db2

  def predict(self,X,W1,W2, b1, b2):

    Z1 = W1.dot(X) + b1
    A1 = self.sigmoid(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = self.sigmoid(Z2)
    return A2

  def update(self,W1, W2, b1, b2,dW1, dW2, db1, db2):

    W1 = W1 - self.lr*dW1
    W2 = W2 - self.lr*dW2
    b1 = b1 - self.lr*db1
    b2 = b2 - self.lr*db2

    return W1, W2, b1, b2


  def fit(self, x_train,y_train, x_test,y_test):
    self.__fit(x_train,y_train, x_test,y_test)

  ## Must pass X_train and Y_train:

  def __fit(self, x_train, y_train, x_test, y_test):

    W1, W2, b1, b2 = self.init_params()
    n_epochs = self.n_epochs

    for i in range(n_epochs):
      ## forward pass
      A2, Z2, A1, Z1 = self.forward_pass(x_train,W1,W2,b1,b2)
      ## backward pass
      dW1, dW2, db1, db2 = self.backward_pass(x_train,y_train,A2,Z2,A1,Z1,W1,W2,b1,b2)
      ## update parameters
      self.W1, self.W2, self.b1, self.b2 = self.update(W1, W2, b1, b2,dW1, dW2, db1, db2)

      ## save the train loss
      self.train_loss.append(self.loss(A2, y_train))
      ## compute test loss
      A2, Z2, A1, Z1 = self.forward_pass(x_test, W1, W2, b1, b2)
      self.test_loss.append(self.loss(A2, y_test))

      ## plot boundary
      if i %1000 == 0:
        self.plot_decision_boundary(self.W1, self.W2, self.b1, self.b2)


    ## plot train and test losses
    plt.plot(self.train_loss)
    plt.plot(self.test_loss)

    ## Get the train and test accuracies:
    y_pred = self.predict(x_train, W1, W2, b1, b2)
    train_accuracy = self.accuracy(y_pred, Y_train)
    print ("train accuracy :", train_accuracy)

    y_pred = self.predict(x_test, W1, W2, b1, b2)
    test_accuracy = self.accuracy(y_pred, y_test)
    print ("test accuracy :", test_accuracy)


  def accuracy(self,y, y_pred):
    pred = (y_pred >= 0.5).astype(int)
    acc = (np.sum(pred == y)/y.shape[1])*100
    return acc

  ## Plot the decision boundary:
  def plot_decision_boundary(self,W1, W2, b1, b2):

    x = np.linspace(-0.5, 2.5,100 )
    y = np.linspace(-0.5, 2.5,100 )
    xv , yv = np.meshgrid(x,y)
    xv.shape , yv.shape
    X_ = np.stack([xv,yv],axis = 0)
    X_ = X_.reshape(2,-1)
    A2, Z2, A1, Z1 = self.forward_pass(X_, W1, W2, b1, b2)
    plt.figure()
    plt.scatter(X_[0,:], X_[1,:], c = A2)
    plt.show()

"""## Generation and split of the data"""

# generate data
var = 0.2
n = 800
class_0_a = var * np.random.randn(n//4,2)
class_0_b =var * np.random.randn(n//4,2) + (2,2)

class_1_a = var* np.random.randn(n//4,2) + (0,2)
class_1_b = var * np.random.randn(n//4,2) +  (2,0)

X = np.concatenate([class_0_a, class_0_b,class_1_a,class_1_b], axis =0)
Y = np.concatenate([np.zeros((n//2,1)), np.ones((n//2,1))])
#X.shape, Y.shape

# shuffle the data
rand_perm = np.random.permutation(n)

X = X[rand_perm, :]
Y = Y[rand_perm, :]

X = X.T
Y = Y.T
#X.shape, Y.shape

# train test split
ratio = 0.8
X_train = X [:, :int (n*ratio)]
Y_train = Y [:, :int (n*ratio)]

X_test = X [:, int (n*ratio):]
Y_test = Y [:, int (n*ratio):]
#X_train.shape

plt.scatter(X_train[0,:], X_train[1,:], c=Y_train[0,:])
plt.show()

"""## Use of the class: neuralNetwork:"""

## Call the class sigmoid:
nn = neuralNetwork(lr = 0.001,n_epochs= 10000)
nn.fit(X_train,Y_train,X_test,Y_test)